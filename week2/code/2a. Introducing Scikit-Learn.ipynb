{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "iris = sns.load_dataset('iris')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal_length  sepal_width  petal_length  petal_width species\n",
       "0           5.1          3.5           1.4          0.2  setosa\n",
       "1           4.9          3.0           1.4          0.2  setosa\n",
       "2           4.7          3.2           1.3          0.2  setosa\n",
       "3           4.6          3.1           1.5          0.2  setosa\n",
       "4           5.0          3.6           1.4          0.2  setosa"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scikit-learn Guiding Principles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Consistency - Sync within the apis\n",
    "* Inspection - Parameters are taken by API. You should be able to configure those parameters\n",
    "* Object Types - The APIs will absord only numpy & dataframes.\n",
    "* Good Defaults - The default params should make good use of scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basics of API\n",
    "1. Choosing the model\n",
    "2. Choosing model hyperparameters - ( init values of model )\n",
    "3. Arrange data into feature matrix (sizes of septels & petels) & target vector ( iris name ) \n",
    "4. Fit/Train the model using fit()\n",
    "5. For supervised learning - Find labels using predict() method\n",
    "6. For unsupervised learning - tranform() predict() to infer properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 3.74540119  9.50714306  7.31993942  5.98658484  1.5601864   1.5599452\n",
      "  0.58083612  8.66176146  6.01115012  7.08072578  0.20584494  9.69909852\n",
      "  8.32442641  2.12339111  1.81824967  1.8340451   3.04242243  5.24756432\n",
      "  4.31945019  2.9122914   6.11852895  1.39493861  2.92144649  3.66361843\n",
      "  4.56069984  7.85175961  1.99673782  5.14234438  5.92414569  0.46450413\n",
      "  6.07544852  1.70524124  0.65051593  9.48885537  9.65632033  8.08397348\n",
      "  3.04613769  0.97672114  6.84233027  4.40152494  1.22038235  4.9517691\n",
      "  0.34388521  9.09320402  2.58779982  6.62522284  3.11711076  5.20068021\n",
      "  5.46710279  1.84854456]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW8AAAD3CAYAAADSftWOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n8BYRiSAFbxGRCFLwFhGJIAVvEZEI+n8f0ANXKIHtWwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1d73e532940>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Simulatibg x & y\n",
    "## This is the training data\n",
    "rng = np.random.RandomState(42)\n",
    "x = 10 * rng.rand(50)\n",
    "print(x)\n",
    "y = 8 * x -2 + rng.randn(50)\n",
    "plt.scatter(x, y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  7.22926896,  18.18565441,  13.52423055,  10.67206599,\n",
       "         0.64185082,   1.4000462 ,  -0.29896653,  17.38064514,\n",
       "        11.36591852,  11.3984114 ,  -0.26422614,  18.01311476,\n",
       "        14.97193082,   3.8584585 ,   3.66749887,   3.59937032,\n",
       "         4.24562734,   9.18591626,   7.9701638 ,   5.80012793,\n",
       "        10.75788366,   1.60421824,   3.736558  ,   5.13103024,\n",
       "         8.93392551,  16.05975926,   2.92146552,  10.28822167,\n",
       "        11.2099274 ,  -0.7161115 ,  11.51229264,   3.94851904,\n",
       "         0.26520582,  19.5423544 ,  15.69289556,  15.98984947,\n",
       "         5.17932245,   0.65443493,  12.77642131,   5.81548096,\n",
       "         1.22109281,   9.26065077,   1.16566447,  16.66813782,\n",
       "         3.36710603,  11.74868864,   6.14962364,   9.73011153,\n",
       "         9.40444538,   3.21035654])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Develop an algorithm which for a value of x will predict value of y\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Choose the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Choose the hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class LinearRegression in module sklearn.linear_model.base:\n",
      "\n",
      "class LinearRegression(LinearModel, sklearn.base.RegressorMixin)\n",
      " |  Ordinary least squares Linear Regression.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  fit_intercept : boolean, optional\n",
      " |      whether to calculate the intercept for this model. If set\n",
      " |      to false, no intercept will be used in calculations\n",
      " |      (e.g. data is expected to be already centered).\n",
      " |  \n",
      " |  normalize : boolean, optional, default False\n",
      " |      If True, the regressors X will be normalized before regression.\n",
      " |      This parameter is ignored when `fit_intercept` is set to False.\n",
      " |      When the regressors are normalized, note that this makes the\n",
      " |      hyperparameters learnt more robust and almost independent of the number\n",
      " |      of samples. The same property is not valid for standardized data.\n",
      " |      However, if you wish to standardize, please use\n",
      " |      `preprocessing.StandardScaler` before calling `fit` on an estimator\n",
      " |      with `normalize=False`.\n",
      " |  \n",
      " |  copy_X : boolean, optional, default True\n",
      " |      If True, X will be copied; else, it may be overwritten.\n",
      " |  \n",
      " |  n_jobs : int, optional, default 1\n",
      " |      The number of jobs to use for the computation.\n",
      " |      If -1 all CPUs are used. This will only provide speedup for\n",
      " |      n_targets > 1 and sufficient large problems.\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  coef_ : array, shape (n_features, ) or (n_targets, n_features)\n",
      " |      Estimated coefficients for the linear regression problem.\n",
      " |      If multiple targets are passed during the fit (y 2D), this\n",
      " |      is a 2D array of shape (n_targets, n_features), while if only\n",
      " |      one target is passed, this is a 1D array of length n_features.\n",
      " |  \n",
      " |  residues_ : array, shape (n_targets,) or (1,) or empty\n",
      " |      Sum of residuals. Squared Euclidean 2-norm for each target passed\n",
      " |      during the fit. If the linear regression problem is under-determined\n",
      " |      (the number of linearly independent rows of the training matrix is less\n",
      " |      than its number of linearly independent columns), this is an empty\n",
      " |      array. If the target vector passed during the fit is 1-dimensional,\n",
      " |      this is a (1,) shape array.\n",
      " |  \n",
      " |      .. versionadded:: 0.18\n",
      " |  \n",
      " |  intercept_ : array\n",
      " |      Independent term in the linear model.\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  From the implementation point of view, this is just plain Ordinary\n",
      " |  Least Squares (scipy.linalg.lstsq) wrapped as a predictor object.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      LinearRegression\n",
      " |      LinearModel\n",
      " |      abc.NewBase\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      sklearn.base.RegressorMixin\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, fit_intercept=True, normalize=False, copy_X=True, n_jobs=1)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  fit(self, X, y, sample_weight=None)\n",
      " |      Fit linear model.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : numpy array or sparse matrix of shape [n_samples,n_features]\n",
      " |          Training data\n",
      " |      \n",
      " |      y : numpy array of shape [n_samples, n_targets]\n",
      " |          Target values\n",
      " |      \n",
      " |      sample_weight : numpy array of shape [n_samples]\n",
      " |          Individual weights for each sample\n",
      " |      \n",
      " |          .. versionadded:: 0.17\n",
      " |             parameter *sample_weight* support to LinearRegression.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : returns an instance of self.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  residues_\n",
      " |      DEPRECATED: ``residues_`` is deprecated and will be removed in 0.19\n",
      " |      \n",
      " |      Get the residues of the fitted model.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from LinearModel:\n",
      " |  \n",
      " |  decision_function(*args, **kwargs)\n",
      " |      DEPRECATED:  and will be removed in 0.19.\n",
      " |      \n",
      " |      Decision function of the linear model.\n",
      " |      \n",
      " |              Parameters\n",
      " |              ----------\n",
      " |              X : {array-like, sparse matrix}, shape = (n_samples, n_features)\n",
      " |                  Samples.\n",
      " |      \n",
      " |              Returns\n",
      " |              -------\n",
      " |              C : array, shape = (n_samples,)\n",
      " |                  Returns predicted values.\n",
      " |  \n",
      " |  predict(self, X)\n",
      " |      Predict using the linear model\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix}, shape = (n_samples, n_features)\n",
      " |          Samples.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      C : array, shape = (n_samples,)\n",
      " |          Returns predicted values.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : boolean, optional\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : mapping of string to any\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as pipelines). The latter have parameters of the form\n",
      " |      ``<component>__<parameter>`` so that it's possible to update each\n",
      " |      component of a nested object.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.RegressorMixin:\n",
      " |  \n",
      " |  score(self, X, y, sample_weight=None)\n",
      " |      Returns the coefficient of determination R^2 of the prediction.\n",
      " |      \n",
      " |      The coefficient R^2 is defined as (1 - u/v), where u is the regression\n",
      " |      sum of squares ((y_true - y_pred) ** 2).sum() and v is the residual\n",
      " |      sum of squares ((y_true - y_true.mean()) ** 2).sum().\n",
      " |      Best possible score is 1.0 and it can be negative (because the\n",
      " |      model can be arbitrarily worse). A constant model that always\n",
      " |      predicts the expected value of y, disregarding the input features,\n",
      " |      would get a R^2 score of 0.0.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape = (n_samples, n_features)\n",
      " |          Test samples.\n",
      " |      \n",
      " |      y : array-like, shape = (n_samples) or (n_samples, n_outputs)\n",
      " |          True values for X.\n",
      " |      \n",
      " |      sample_weight : array-like, shape = [n_samples], optional\n",
      " |          Sample weights.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |          R^2 of self.predict(X) wrt. y.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(LinearRegression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = LinearRegression(fit_intercept=True) # Adding a hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Independent Variable\n",
    "Not dependent on other variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dependent Variables\n",
    "Dependent on other variables - house price"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Get Feature Matrix & target vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3.74540119],\n",
       "       [ 9.50714306],\n",
       "       [ 7.31993942],\n",
       "       [ 5.98658484],\n",
       "       [ 1.5601864 ],\n",
       "       [ 1.5599452 ],\n",
       "       [ 0.58083612],\n",
       "       [ 8.66176146],\n",
       "       [ 6.01115012],\n",
       "       [ 7.08072578],\n",
       "       [ 0.20584494],\n",
       "       [ 9.69909852],\n",
       "       [ 8.32442641],\n",
       "       [ 2.12339111],\n",
       "       [ 1.81824967],\n",
       "       [ 1.8340451 ],\n",
       "       [ 3.04242243],\n",
       "       [ 5.24756432],\n",
       "       [ 4.31945019],\n",
       "       [ 2.9122914 ],\n",
       "       [ 6.11852895],\n",
       "       [ 1.39493861],\n",
       "       [ 2.92144649],\n",
       "       [ 3.66361843],\n",
       "       [ 4.56069984],\n",
       "       [ 7.85175961],\n",
       "       [ 1.99673782],\n",
       "       [ 5.14234438],\n",
       "       [ 5.92414569],\n",
       "       [ 0.46450413],\n",
       "       [ 6.07544852],\n",
       "       [ 1.70524124],\n",
       "       [ 0.65051593],\n",
       "       [ 9.48885537],\n",
       "       [ 9.65632033],\n",
       "       [ 8.08397348],\n",
       "       [ 3.04613769],\n",
       "       [ 0.97672114],\n",
       "       [ 6.84233027],\n",
       "       [ 4.40152494],\n",
       "       [ 1.22038235],\n",
       "       [ 4.9517691 ],\n",
       "       [ 0.34388521],\n",
       "       [ 9.09320402],\n",
       "       [ 2.58779982],\n",
       "       [ 6.62522284],\n",
       "       [ 3.11711076],\n",
       "       [ 5.20068021],\n",
       "       [ 5.46710279],\n",
       "       [ 1.84854456]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = x[:, np.newaxis]\n",
    "\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 1)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X,y) # model traning, model object is now a learned entity\n",
    "# The model is trained\n",
    "# This results into a function which in future if passed with feature matrix, will yield target vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 7.9776566]\n",
      "-1.90331072553\n"
     ]
    }
   ],
   "source": [
    "#Model is the realationship holder/finder between X & y\n",
    "#Find line which best fit the dataset\n",
    "print(model.coef_)\n",
    "print(model.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Predict outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10,)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newx = np.linspace(11,20,10)\n",
    "newy = model.predict(newx[:, np.newaxis])\n",
    "newx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x20026e97a58>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD3CAYAAADmBxSSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x20026e48da0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(newx,newy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.9776566])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.90331072553111635"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3.74540119],\n",
       "       [ 9.50714306],\n",
       "       [ 7.31993942],\n",
       "       [ 5.98658484],\n",
       "       [ 1.5601864 ],\n",
       "       [ 1.5599452 ],\n",
       "       [ 0.58083612],\n",
       "       [ 8.66176146],\n",
       "       [ 6.01115012],\n",
       "       [ 7.08072578],\n",
       "       [ 0.20584494],\n",
       "       [ 9.69909852],\n",
       "       [ 8.32442641],\n",
       "       [ 2.12339111],\n",
       "       [ 1.81824967],\n",
       "       [ 1.8340451 ],\n",
       "       [ 3.04242243],\n",
       "       [ 5.24756432],\n",
       "       [ 4.31945019],\n",
       "       [ 2.9122914 ],\n",
       "       [ 6.11852895],\n",
       "       [ 1.39493861],\n",
       "       [ 2.92144649],\n",
       "       [ 3.66361843],\n",
       "       [ 4.56069984],\n",
       "       [ 7.85175961],\n",
       "       [ 1.99673782],\n",
       "       [ 5.14234438],\n",
       "       [ 5.92414569],\n",
       "       [ 0.46450413],\n",
       "       [ 6.07544852],\n",
       "       [ 1.70524124],\n",
       "       [ 0.65051593],\n",
       "       [ 9.48885537],\n",
       "       [ 9.65632033],\n",
       "       [ 8.08397348],\n",
       "       [ 3.04613769],\n",
       "       [ 0.97672114],\n",
       "       [ 6.84233027],\n",
       "       [ 4.40152494],\n",
       "       [ 1.22038235],\n",
       "       [ 4.9517691 ],\n",
       "       [ 0.34388521],\n",
       "       [ 9.09320402],\n",
       "       [ 2.58779982],\n",
       "       [ 6.62522284],\n",
       "       [ 3.11711076],\n",
       "       [ 5.20068021],\n",
       "       [ 5.46710279],\n",
       "       [ 1.84854456]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[:, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5.4</td>\n",
       "      <td>3.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4.4</td>\n",
       "      <td>2.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>5.4</td>\n",
       "      <td>3.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>4.8</td>\n",
       "      <td>3.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>4.8</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>4.3</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>5.8</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>5.7</td>\n",
       "      <td>4.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>5.4</td>\n",
       "      <td>3.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>5.7</td>\n",
       "      <td>3.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>5.4</td>\n",
       "      <td>3.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>4.8</td>\n",
       "      <td>3.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>5.2</td>\n",
       "      <td>3.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>5.2</td>\n",
       "      <td>3.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>6.9</td>\n",
       "      <td>3.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>5.6</td>\n",
       "      <td>2.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>7.7</td>\n",
       "      <td>2.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>7.2</td>\n",
       "      <td>3.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>6.2</td>\n",
       "      <td>2.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>6.1</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>6.4</td>\n",
       "      <td>2.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>7.2</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>7.4</td>\n",
       "      <td>2.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>7.9</td>\n",
       "      <td>3.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>6.4</td>\n",
       "      <td>2.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>6.1</td>\n",
       "      <td>2.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>7.7</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>6.3</td>\n",
       "      <td>3.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>6.4</td>\n",
       "      <td>3.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>6.9</td>\n",
       "      <td>3.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>6.9</td>\n",
       "      <td>3.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>5.8</td>\n",
       "      <td>2.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>6.8</td>\n",
       "      <td>3.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>6.5</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>6.2</td>\n",
       "      <td>3.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>5.9</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows  2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     sepal_length  sepal_width\n",
       "0             5.1          3.5\n",
       "1             4.9          3.0\n",
       "2             4.7          3.2\n",
       "3             4.6          3.1\n",
       "4             5.0          3.6\n",
       "5             5.4          3.9\n",
       "6             4.6          3.4\n",
       "7             5.0          3.4\n",
       "8             4.4          2.9\n",
       "9             4.9          3.1\n",
       "10            5.4          3.7\n",
       "11            4.8          3.4\n",
       "12            4.8          3.0\n",
       "13            4.3          3.0\n",
       "14            5.8          4.0\n",
       "15            5.7          4.4\n",
       "16            5.4          3.9\n",
       "17            5.1          3.5\n",
       "18            5.7          3.8\n",
       "19            5.1          3.8\n",
       "20            5.4          3.4\n",
       "21            5.1          3.7\n",
       "22            4.6          3.6\n",
       "23            5.1          3.3\n",
       "24            4.8          3.4\n",
       "25            5.0          3.0\n",
       "26            5.0          3.4\n",
       "27            5.2          3.5\n",
       "28            5.2          3.4\n",
       "29            4.7          3.2\n",
       "..            ...          ...\n",
       "120           6.9          3.2\n",
       "121           5.6          2.8\n",
       "122           7.7          2.8\n",
       "123           6.3          2.7\n",
       "124           6.7          3.3\n",
       "125           7.2          3.2\n",
       "126           6.2          2.8\n",
       "127           6.1          3.0\n",
       "128           6.4          2.8\n",
       "129           7.2          3.0\n",
       "130           7.4          2.8\n",
       "131           7.9          3.8\n",
       "132           6.4          2.8\n",
       "133           6.3          2.8\n",
       "134           6.1          2.6\n",
       "135           7.7          3.0\n",
       "136           6.3          3.4\n",
       "137           6.4          3.1\n",
       "138           6.0          3.0\n",
       "139           6.9          3.1\n",
       "140           6.7          3.1\n",
       "141           6.9          3.1\n",
       "142           5.8          2.7\n",
       "143           6.8          3.2\n",
       "144           6.7          3.3\n",
       "145           6.7          3.0\n",
       "146           6.3          2.5\n",
       "147           6.5          3.0\n",
       "148           6.2          3.4\n",
       "149           5.9          3.0\n",
       "\n",
       "[150 rows x 2 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris[['sepal_length','sepal_width']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([18, 29, 19, 10, 20, 18, 27, 14, 16, 20, 23, 16, 12, 20, 26, 18, 10,\n",
       "       23, 27, 16, 11, 16, 23, 20, 10, 23, 27, 21, 22, 12, 21, 23, 16, 23,\n",
       "       26, 20, 26, 15, 11, 23, 29, 25, 10, 27, 20, 23, 13, 22, 23, 26, 10,\n",
       "       21, 24, 25, 18, 18, 17, 25, 11, 21, 18, 27, 21, 25, 23, 22, 22, 25,\n",
       "       11, 13, 18, 10, 28, 24, 14, 16, 17, 21, 28, 24, 26, 25, 13, 10, 25,\n",
       "       13, 29, 17, 22, 19, 16, 19, 15, 28, 29, 15, 22, 18, 25, 12])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.random.randint(low=10, high=30, size=100)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[41, 63],\n",
       "       [43, 25],\n",
       "       [45, 41],\n",
       "       [59, 33],\n",
       "       [37, 45],\n",
       "       [51, 37],\n",
       "       [29, 45],\n",
       "       [57, 41],\n",
       "       [25, 51],\n",
       "       [59, 37],\n",
       "       [27, 37],\n",
       "       [51, 45],\n",
       "       [25, 51],\n",
       "       [59, 47],\n",
       "       [49, 29],\n",
       "       [47, 51],\n",
       "       [37, 51],\n",
       "       [57, 45],\n",
       "       [57, 35],\n",
       "       [27, 51],\n",
       "       [63, 55],\n",
       "       [25, 59],\n",
       "       [45, 51],\n",
       "       [31, 49],\n",
       "       [51, 57],\n",
       "       [25, 47],\n",
       "       [53, 55],\n",
       "       [41, 41],\n",
       "       [39, 55],\n",
       "       [27, 47],\n",
       "       [41, 59],\n",
       "       [47, 55],\n",
       "       [51, 49],\n",
       "       [49, 55],\n",
       "       [27, 31],\n",
       "       [41, 25],\n",
       "       [61, 53],\n",
       "       [33, 37],\n",
       "       [39, 47],\n",
       "       [61, 53],\n",
       "       [57, 55],\n",
       "       [31, 25],\n",
       "       [55, 31],\n",
       "       [63, 39],\n",
       "       [49, 43],\n",
       "       [37, 43],\n",
       "       [35, 61],\n",
       "       [63, 35],\n",
       "       [49, 41],\n",
       "       [55, 29]])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = x.reshape(50,2)\n",
    "y = X * 2 + 5\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y = y.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([104,  68,  86,  92,  82,  88,  74,  98,  76,  96,  64,  96,  76,\n",
       "       106,  78,  98,  88, 102,  92,  78, 118,  84,  96,  80, 108,  72,\n",
       "       108,  82,  94,  74, 100, 102, 100, 104,  58,  66, 114,  70,  86,\n",
       "       114, 112,  56,  86, 102,  92,  80,  96,  98,  90,  84])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[18, 29],\n",
       "       [19, 10],\n",
       "       [20, 18],\n",
       "       [27, 14],\n",
       "       [16, 20],\n",
       "       [23, 16],\n",
       "       [12, 20],\n",
       "       [26, 18],\n",
       "       [10, 23],\n",
       "       [27, 16],\n",
       "       [11, 16],\n",
       "       [23, 20],\n",
       "       [10, 23],\n",
       "       [27, 21],\n",
       "       [22, 12],\n",
       "       [21, 23],\n",
       "       [16, 23],\n",
       "       [26, 20],\n",
       "       [26, 15],\n",
       "       [11, 23],\n",
       "       [29, 25],\n",
       "       [10, 27],\n",
       "       [20, 23],\n",
       "       [13, 22],\n",
       "       [23, 26],\n",
       "       [10, 21],\n",
       "       [24, 25],\n",
       "       [18, 18],\n",
       "       [17, 25],\n",
       "       [11, 21],\n",
       "       [18, 27],\n",
       "       [21, 25],\n",
       "       [23, 22],\n",
       "       [22, 25],\n",
       "       [11, 13],\n",
       "       [18, 10],\n",
       "       [28, 24],\n",
       "       [14, 16],\n",
       "       [17, 21],\n",
       "       [28, 24],\n",
       "       [26, 25],\n",
       "       [13, 10],\n",
       "       [25, 13],\n",
       "       [29, 17],\n",
       "       [22, 19],\n",
       "       [16, 19],\n",
       "       [15, 28],\n",
       "       [29, 15],\n",
       "       [22, 18],\n",
       "       [25, 12]])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 54,  72, 106,  92,  80, 104,  96, 104,  86,  76,  98,  90,  96,\n",
       "        52,  62,  82,  98,  64,  82,  90,  72,  86, 100,  56,  58, 100,\n",
       "        72, 108,  88,  96, 106,  86,  54, 116,  56, 104,  64,  84,  90,\n",
       "        88,  96, 100,  88,  86,  82,  96,  96,  80,  84,  92])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([11, 11, 21, 10, 24, 24, 24, 17, 24, 11, 23, 24, 23, 20, 25, 22, 20,\n",
       "       18, 23, 10, 27, 17, 20, 20, 27, 16, 11, 10, 12, 14, 26, 10, 29, 15,\n",
       "       12, 15, 23, 13, 21, 19, 19, 12, 22, 16, 20, 25, 13, 10, 10, 14, 16,\n",
       "       29, 13, 18, 27, 22, 17, 22, 18, 25, 27, 21, 13, 25, 10, 12, 29, 24,\n",
       "       10, 13, 18, 29, 14, 13, 16, 21, 15, 25, 17, 22, 19, 24, 21, 24, 22,\n",
       "       17, 25, 13, 17, 19, 22, 21, 18, 25, 19, 16, 13, 24, 18, 23])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression(fit_intercept=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([11, 11, 21, 10, 24, 24, 24, 17, 24, 11, 23, 24, 23, 20, 25, 22, 20,\n",
       "       18, 23, 10, 27, 17, 20, 20, 27, 16, 11, 10, 12, 14, 26, 10, 29, 15,\n",
       "       12, 15, 23, 13, 21, 19, 19, 12, 22, 16, 20, 25, 13, 10, 10, 14, 16,\n",
       "       29, 13, 18, 27, 22, 17, 22, 18, 25, 27, 21, 13, 25, 10, 12, 29, 24,\n",
       "       10, 13, 18, 29, 14, 13, 16, 21, 15, 25, 17, 22, 19, 24, 21, 24, 22,\n",
       "       17, 25, 13, 17, 19, 22, 21, 18, 25, 19, 16, 13, 24, 18, 23])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.,  2.])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.9999999999999858"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 164.,  124.])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict([[33,44],[55,2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "ename": "HTTPError",
     "evalue": "HTTP Error 404: Not Found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m-----------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-76-22d144297a9a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0miris\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'boston'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Users\\ZekeLabs\\Anaconda3\\lib\\site-packages\\seaborn\\utils.py\u001b[0m in \u001b[0;36mload_dataset\u001b[1;34m(name, cache, data_home, **kws)\u001b[0m\n\u001b[0;32m    422\u001b[0m                                   os.path.basename(full_path))\n\u001b[0;32m    423\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcache_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 424\u001b[1;33m             \u001b[0murlretrieve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfull_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcache_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    425\u001b[0m         \u001b[0mfull_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcache_path\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    426\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\ZekeLabs\\Anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36murlretrieve\u001b[1;34m(url, filename, reporthook, data)\u001b[0m\n\u001b[0;32m    185\u001b[0m     \u001b[0murl_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msplittype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    186\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 187\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mcontextlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclosing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murlopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    188\u001b[0m         \u001b[0mheaders\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    189\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\ZekeLabs\\Anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36murlopen\u001b[1;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[0;32m    160\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    161\u001b[0m         \u001b[0mopener\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_opener\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 162\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mopener\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    163\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    164\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0minstall_opener\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopener\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\ZekeLabs\\Anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[0;32m    469\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mprocessor\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocess_response\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    470\u001b[0m             \u001b[0mmeth\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 471\u001b[1;33m             \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmeth\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    472\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    473\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\ZekeLabs\\Anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36mhttp_response\u001b[1;34m(self, request, response)\u001b[0m\n\u001b[0;32m    579\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m200\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[0mcode\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m300\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    580\u001b[0m             response = self.parent.error(\n\u001b[1;32m--> 581\u001b[1;33m                 'http', request, response, code, msg, hdrs)\n\u001b[0m\u001b[0;32m    582\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    583\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\ZekeLabs\\Anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36merror\u001b[1;34m(self, proto, *args)\u001b[0m\n\u001b[0;32m    507\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhttp_err\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    508\u001b[0m             \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'default'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'http_error_default'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0morig_args\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 509\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_chain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    510\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    511\u001b[0m \u001b[1;31m# XXX probably also want an abstract factory that knows when it makes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\ZekeLabs\\Anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36m_call_chain\u001b[1;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[0;32m    441\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhandler\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mhandlers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    442\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandler\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 443\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    444\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    445\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\ZekeLabs\\Anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36mhttp_error_default\u001b[1;34m(self, req, fp, code, msg, hdrs)\u001b[0m\n\u001b[0;32m    587\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mHTTPDefaultErrorHandler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBaseHandler\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    588\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mhttp_error_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhdrs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 589\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfull_url\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhdrs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    590\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    591\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mHTTPRedirectHandler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBaseHandler\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mHTTPError\u001b[0m: HTTP Error 404: Not Found"
     ]
    }
   ],
   "source": [
    "\n",
    "model.score()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'name':['a','b','c'],'age':[44,33,11]})\n",
    "y = np.array([5,66,77])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a,b,c,d = train_test_split(df,y, test_size=0.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>44</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11</td>\n",
       "      <td>c</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age name\n",
       "0   44    a\n",
       "2   11    c"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>33</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age name\n",
       "1   33    b"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5, 77])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([66])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "boston_data = load_boston()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD',\n",
       "       'TAX', 'PTRATIO', 'B', 'LSTAT'], \n",
       "      dtype='<U7')"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boston.feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(506, 13)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boston.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston = pd.DataFrame(boston.data,columns=['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD',\n",
    "       'TAX', 'PTRATIO', 'B', 'LSTAT'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD',\n",
       "       'TAX', 'PTRATIO', 'B', 'LSTAT'], \n",
       "      dtype='<U7')"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boston.feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1.0</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.02985</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.430</td>\n",
       "      <td>58.7</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.12</td>\n",
       "      <td>5.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.08829</td>\n",
       "      <td>12.5</td>\n",
       "      <td>7.87</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.524</td>\n",
       "      <td>6.012</td>\n",
       "      <td>66.6</td>\n",
       "      <td>5.5605</td>\n",
       "      <td>5.0</td>\n",
       "      <td>311.0</td>\n",
       "      <td>15.2</td>\n",
       "      <td>395.60</td>\n",
       "      <td>12.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.14455</td>\n",
       "      <td>12.5</td>\n",
       "      <td>7.87</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.524</td>\n",
       "      <td>6.172</td>\n",
       "      <td>96.1</td>\n",
       "      <td>5.9505</td>\n",
       "      <td>5.0</td>\n",
       "      <td>311.0</td>\n",
       "      <td>15.2</td>\n",
       "      <td>396.90</td>\n",
       "      <td>19.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.21124</td>\n",
       "      <td>12.5</td>\n",
       "      <td>7.87</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.524</td>\n",
       "      <td>5.631</td>\n",
       "      <td>100.0</td>\n",
       "      <td>6.0821</td>\n",
       "      <td>5.0</td>\n",
       "      <td>311.0</td>\n",
       "      <td>15.2</td>\n",
       "      <td>386.63</td>\n",
       "      <td>29.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.17004</td>\n",
       "      <td>12.5</td>\n",
       "      <td>7.87</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.524</td>\n",
       "      <td>6.004</td>\n",
       "      <td>85.9</td>\n",
       "      <td>6.5921</td>\n",
       "      <td>5.0</td>\n",
       "      <td>311.0</td>\n",
       "      <td>15.2</td>\n",
       "      <td>386.71</td>\n",
       "      <td>17.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.22489</td>\n",
       "      <td>12.5</td>\n",
       "      <td>7.87</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.524</td>\n",
       "      <td>6.377</td>\n",
       "      <td>94.3</td>\n",
       "      <td>6.3467</td>\n",
       "      <td>5.0</td>\n",
       "      <td>311.0</td>\n",
       "      <td>15.2</td>\n",
       "      <td>392.52</td>\n",
       "      <td>20.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.11747</td>\n",
       "      <td>12.5</td>\n",
       "      <td>7.87</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.524</td>\n",
       "      <td>6.009</td>\n",
       "      <td>82.9</td>\n",
       "      <td>6.2267</td>\n",
       "      <td>5.0</td>\n",
       "      <td>311.0</td>\n",
       "      <td>15.2</td>\n",
       "      <td>396.90</td>\n",
       "      <td>13.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.09378</td>\n",
       "      <td>12.5</td>\n",
       "      <td>7.87</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.524</td>\n",
       "      <td>5.889</td>\n",
       "      <td>39.0</td>\n",
       "      <td>5.4509</td>\n",
       "      <td>5.0</td>\n",
       "      <td>311.0</td>\n",
       "      <td>15.2</td>\n",
       "      <td>390.50</td>\n",
       "      <td>15.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.62976</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>5.949</td>\n",
       "      <td>61.8</td>\n",
       "      <td>4.7075</td>\n",
       "      <td>4.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>396.90</td>\n",
       "      <td>8.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.63796</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.096</td>\n",
       "      <td>84.5</td>\n",
       "      <td>4.4619</td>\n",
       "      <td>4.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>380.02</td>\n",
       "      <td>10.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.62739</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>5.834</td>\n",
       "      <td>56.5</td>\n",
       "      <td>4.4986</td>\n",
       "      <td>4.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>395.62</td>\n",
       "      <td>8.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1.05393</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>5.935</td>\n",
       "      <td>29.3</td>\n",
       "      <td>4.4986</td>\n",
       "      <td>4.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>386.85</td>\n",
       "      <td>6.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.78420</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>5.990</td>\n",
       "      <td>81.7</td>\n",
       "      <td>4.2579</td>\n",
       "      <td>4.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>386.75</td>\n",
       "      <td>14.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.80271</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>5.456</td>\n",
       "      <td>36.6</td>\n",
       "      <td>3.7965</td>\n",
       "      <td>4.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>288.99</td>\n",
       "      <td>11.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.72580</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>5.727</td>\n",
       "      <td>69.5</td>\n",
       "      <td>3.7965</td>\n",
       "      <td>4.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>390.95</td>\n",
       "      <td>11.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1.25179</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>5.570</td>\n",
       "      <td>98.1</td>\n",
       "      <td>3.7979</td>\n",
       "      <td>4.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>376.57</td>\n",
       "      <td>21.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.85204</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>5.965</td>\n",
       "      <td>89.2</td>\n",
       "      <td>4.0123</td>\n",
       "      <td>4.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>392.53</td>\n",
       "      <td>13.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1.23247</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.142</td>\n",
       "      <td>91.7</td>\n",
       "      <td>3.9769</td>\n",
       "      <td>4.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>396.90</td>\n",
       "      <td>18.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.98843</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>5.813</td>\n",
       "      <td>100.0</td>\n",
       "      <td>4.0952</td>\n",
       "      <td>4.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>394.54</td>\n",
       "      <td>19.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.75026</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>5.924</td>\n",
       "      <td>94.1</td>\n",
       "      <td>4.3996</td>\n",
       "      <td>4.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>394.33</td>\n",
       "      <td>16.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.84054</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>5.599</td>\n",
       "      <td>85.7</td>\n",
       "      <td>4.4546</td>\n",
       "      <td>4.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>303.42</td>\n",
       "      <td>16.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.67191</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>5.813</td>\n",
       "      <td>90.3</td>\n",
       "      <td>4.6820</td>\n",
       "      <td>4.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>376.88</td>\n",
       "      <td>14.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.95577</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.047</td>\n",
       "      <td>88.8</td>\n",
       "      <td>4.4534</td>\n",
       "      <td>4.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>306.38</td>\n",
       "      <td>17.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.77299</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.495</td>\n",
       "      <td>94.4</td>\n",
       "      <td>4.4547</td>\n",
       "      <td>4.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>387.94</td>\n",
       "      <td>12.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1.00245</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.674</td>\n",
       "      <td>87.3</td>\n",
       "      <td>4.2390</td>\n",
       "      <td>4.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>380.23</td>\n",
       "      <td>11.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>476</th>\n",
       "      <td>4.87141</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.614</td>\n",
       "      <td>6.484</td>\n",
       "      <td>93.6</td>\n",
       "      <td>2.3053</td>\n",
       "      <td>24.0</td>\n",
       "      <td>666.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>396.21</td>\n",
       "      <td>18.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>477</th>\n",
       "      <td>15.02340</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.614</td>\n",
       "      <td>5.304</td>\n",
       "      <td>97.3</td>\n",
       "      <td>2.1007</td>\n",
       "      <td>24.0</td>\n",
       "      <td>666.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>349.48</td>\n",
       "      <td>24.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>478</th>\n",
       "      <td>10.23300</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.614</td>\n",
       "      <td>6.185</td>\n",
       "      <td>96.7</td>\n",
       "      <td>2.1705</td>\n",
       "      <td>24.0</td>\n",
       "      <td>666.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>379.70</td>\n",
       "      <td>18.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>479</th>\n",
       "      <td>14.33370</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.614</td>\n",
       "      <td>6.229</td>\n",
       "      <td>88.0</td>\n",
       "      <td>1.9512</td>\n",
       "      <td>24.0</td>\n",
       "      <td>666.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>383.32</td>\n",
       "      <td>13.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>480</th>\n",
       "      <td>5.82401</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.532</td>\n",
       "      <td>6.242</td>\n",
       "      <td>64.7</td>\n",
       "      <td>3.4242</td>\n",
       "      <td>24.0</td>\n",
       "      <td>666.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>396.90</td>\n",
       "      <td>10.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>481</th>\n",
       "      <td>5.70818</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.532</td>\n",
       "      <td>6.750</td>\n",
       "      <td>74.9</td>\n",
       "      <td>3.3317</td>\n",
       "      <td>24.0</td>\n",
       "      <td>666.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>393.07</td>\n",
       "      <td>7.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>482</th>\n",
       "      <td>5.73116</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.532</td>\n",
       "      <td>7.061</td>\n",
       "      <td>77.0</td>\n",
       "      <td>3.4106</td>\n",
       "      <td>24.0</td>\n",
       "      <td>666.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>395.28</td>\n",
       "      <td>7.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>483</th>\n",
       "      <td>2.81838</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.532</td>\n",
       "      <td>5.762</td>\n",
       "      <td>40.3</td>\n",
       "      <td>4.0983</td>\n",
       "      <td>24.0</td>\n",
       "      <td>666.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>392.92</td>\n",
       "      <td>10.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>484</th>\n",
       "      <td>2.37857</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.583</td>\n",
       "      <td>5.871</td>\n",
       "      <td>41.9</td>\n",
       "      <td>3.7240</td>\n",
       "      <td>24.0</td>\n",
       "      <td>666.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>370.73</td>\n",
       "      <td>13.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>485</th>\n",
       "      <td>3.67367</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.583</td>\n",
       "      <td>6.312</td>\n",
       "      <td>51.9</td>\n",
       "      <td>3.9917</td>\n",
       "      <td>24.0</td>\n",
       "      <td>666.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>388.62</td>\n",
       "      <td>10.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>486</th>\n",
       "      <td>5.69175</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.583</td>\n",
       "      <td>6.114</td>\n",
       "      <td>79.8</td>\n",
       "      <td>3.5459</td>\n",
       "      <td>24.0</td>\n",
       "      <td>666.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>392.68</td>\n",
       "      <td>14.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>487</th>\n",
       "      <td>4.83567</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.583</td>\n",
       "      <td>5.905</td>\n",
       "      <td>53.2</td>\n",
       "      <td>3.1523</td>\n",
       "      <td>24.0</td>\n",
       "      <td>666.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>388.22</td>\n",
       "      <td>11.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>488</th>\n",
       "      <td>0.15086</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.74</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.609</td>\n",
       "      <td>5.454</td>\n",
       "      <td>92.7</td>\n",
       "      <td>1.8209</td>\n",
       "      <td>4.0</td>\n",
       "      <td>711.0</td>\n",
       "      <td>20.1</td>\n",
       "      <td>395.09</td>\n",
       "      <td>18.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>489</th>\n",
       "      <td>0.18337</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.74</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.609</td>\n",
       "      <td>5.414</td>\n",
       "      <td>98.3</td>\n",
       "      <td>1.7554</td>\n",
       "      <td>4.0</td>\n",
       "      <td>711.0</td>\n",
       "      <td>20.1</td>\n",
       "      <td>344.05</td>\n",
       "      <td>23.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>490</th>\n",
       "      <td>0.20746</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.74</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.609</td>\n",
       "      <td>5.093</td>\n",
       "      <td>98.0</td>\n",
       "      <td>1.8226</td>\n",
       "      <td>4.0</td>\n",
       "      <td>711.0</td>\n",
       "      <td>20.1</td>\n",
       "      <td>318.43</td>\n",
       "      <td>29.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>491</th>\n",
       "      <td>0.10574</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.74</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.609</td>\n",
       "      <td>5.983</td>\n",
       "      <td>98.8</td>\n",
       "      <td>1.8681</td>\n",
       "      <td>4.0</td>\n",
       "      <td>711.0</td>\n",
       "      <td>20.1</td>\n",
       "      <td>390.11</td>\n",
       "      <td>18.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>492</th>\n",
       "      <td>0.11132</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.74</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.609</td>\n",
       "      <td>5.983</td>\n",
       "      <td>83.5</td>\n",
       "      <td>2.1099</td>\n",
       "      <td>4.0</td>\n",
       "      <td>711.0</td>\n",
       "      <td>20.1</td>\n",
       "      <td>396.90</td>\n",
       "      <td>13.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>493</th>\n",
       "      <td>0.17331</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.69</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.585</td>\n",
       "      <td>5.707</td>\n",
       "      <td>54.0</td>\n",
       "      <td>2.3817</td>\n",
       "      <td>6.0</td>\n",
       "      <td>391.0</td>\n",
       "      <td>19.2</td>\n",
       "      <td>396.90</td>\n",
       "      <td>12.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494</th>\n",
       "      <td>0.27957</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.69</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.585</td>\n",
       "      <td>5.926</td>\n",
       "      <td>42.6</td>\n",
       "      <td>2.3817</td>\n",
       "      <td>6.0</td>\n",
       "      <td>391.0</td>\n",
       "      <td>19.2</td>\n",
       "      <td>396.90</td>\n",
       "      <td>13.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>0.17899</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.69</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.585</td>\n",
       "      <td>5.670</td>\n",
       "      <td>28.8</td>\n",
       "      <td>2.7986</td>\n",
       "      <td>6.0</td>\n",
       "      <td>391.0</td>\n",
       "      <td>19.2</td>\n",
       "      <td>393.29</td>\n",
       "      <td>17.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>0.28960</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.69</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.585</td>\n",
       "      <td>5.390</td>\n",
       "      <td>72.9</td>\n",
       "      <td>2.7986</td>\n",
       "      <td>6.0</td>\n",
       "      <td>391.0</td>\n",
       "      <td>19.2</td>\n",
       "      <td>396.90</td>\n",
       "      <td>21.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>0.26838</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.69</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.585</td>\n",
       "      <td>5.794</td>\n",
       "      <td>70.6</td>\n",
       "      <td>2.8927</td>\n",
       "      <td>6.0</td>\n",
       "      <td>391.0</td>\n",
       "      <td>19.2</td>\n",
       "      <td>396.90</td>\n",
       "      <td>14.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>0.23912</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.69</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.585</td>\n",
       "      <td>6.019</td>\n",
       "      <td>65.3</td>\n",
       "      <td>2.4091</td>\n",
       "      <td>6.0</td>\n",
       "      <td>391.0</td>\n",
       "      <td>19.2</td>\n",
       "      <td>396.90</td>\n",
       "      <td>12.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>0.17783</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.69</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.585</td>\n",
       "      <td>5.569</td>\n",
       "      <td>73.5</td>\n",
       "      <td>2.3999</td>\n",
       "      <td>6.0</td>\n",
       "      <td>391.0</td>\n",
       "      <td>19.2</td>\n",
       "      <td>395.77</td>\n",
       "      <td>15.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>0.22438</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.69</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.585</td>\n",
       "      <td>6.027</td>\n",
       "      <td>79.7</td>\n",
       "      <td>2.4982</td>\n",
       "      <td>6.0</td>\n",
       "      <td>391.0</td>\n",
       "      <td>19.2</td>\n",
       "      <td>396.90</td>\n",
       "      <td>14.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>501</th>\n",
       "      <td>0.06263</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.593</td>\n",
       "      <td>69.1</td>\n",
       "      <td>2.4786</td>\n",
       "      <td>1.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>391.99</td>\n",
       "      <td>9.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>502</th>\n",
       "      <td>0.04527</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.120</td>\n",
       "      <td>76.7</td>\n",
       "      <td>2.2875</td>\n",
       "      <td>1.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>503</th>\n",
       "      <td>0.06076</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.976</td>\n",
       "      <td>91.0</td>\n",
       "      <td>2.1675</td>\n",
       "      <td>1.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>504</th>\n",
       "      <td>0.10959</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.794</td>\n",
       "      <td>89.3</td>\n",
       "      <td>2.3889</td>\n",
       "      <td>1.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>393.45</td>\n",
       "      <td>6.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>505</th>\n",
       "      <td>0.04741</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.030</td>\n",
       "      <td>80.8</td>\n",
       "      <td>2.5050</td>\n",
       "      <td>1.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>396.90</td>\n",
       "      <td>7.88</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>506 rows  13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         CRIM    ZN  INDUS  CHAS    NOX     RM    AGE     DIS   RAD    TAX  \\\n",
       "0     0.00632  18.0   2.31   0.0  0.538  6.575   65.2  4.0900   1.0  296.0   \n",
       "1     0.02731   0.0   7.07   0.0  0.469  6.421   78.9  4.9671   2.0  242.0   \n",
       "2     0.02729   0.0   7.07   0.0  0.469  7.185   61.1  4.9671   2.0  242.0   \n",
       "3     0.03237   0.0   2.18   0.0  0.458  6.998   45.8  6.0622   3.0  222.0   \n",
       "4     0.06905   0.0   2.18   0.0  0.458  7.147   54.2  6.0622   3.0  222.0   \n",
       "5     0.02985   0.0   2.18   0.0  0.458  6.430   58.7  6.0622   3.0  222.0   \n",
       "6     0.08829  12.5   7.87   0.0  0.524  6.012   66.6  5.5605   5.0  311.0   \n",
       "7     0.14455  12.5   7.87   0.0  0.524  6.172   96.1  5.9505   5.0  311.0   \n",
       "8     0.21124  12.5   7.87   0.0  0.524  5.631  100.0  6.0821   5.0  311.0   \n",
       "9     0.17004  12.5   7.87   0.0  0.524  6.004   85.9  6.5921   5.0  311.0   \n",
       "10    0.22489  12.5   7.87   0.0  0.524  6.377   94.3  6.3467   5.0  311.0   \n",
       "11    0.11747  12.5   7.87   0.0  0.524  6.009   82.9  6.2267   5.0  311.0   \n",
       "12    0.09378  12.5   7.87   0.0  0.524  5.889   39.0  5.4509   5.0  311.0   \n",
       "13    0.62976   0.0   8.14   0.0  0.538  5.949   61.8  4.7075   4.0  307.0   \n",
       "14    0.63796   0.0   8.14   0.0  0.538  6.096   84.5  4.4619   4.0  307.0   \n",
       "15    0.62739   0.0   8.14   0.0  0.538  5.834   56.5  4.4986   4.0  307.0   \n",
       "16    1.05393   0.0   8.14   0.0  0.538  5.935   29.3  4.4986   4.0  307.0   \n",
       "17    0.78420   0.0   8.14   0.0  0.538  5.990   81.7  4.2579   4.0  307.0   \n",
       "18    0.80271   0.0   8.14   0.0  0.538  5.456   36.6  3.7965   4.0  307.0   \n",
       "19    0.72580   0.0   8.14   0.0  0.538  5.727   69.5  3.7965   4.0  307.0   \n",
       "20    1.25179   0.0   8.14   0.0  0.538  5.570   98.1  3.7979   4.0  307.0   \n",
       "21    0.85204   0.0   8.14   0.0  0.538  5.965   89.2  4.0123   4.0  307.0   \n",
       "22    1.23247   0.0   8.14   0.0  0.538  6.142   91.7  3.9769   4.0  307.0   \n",
       "23    0.98843   0.0   8.14   0.0  0.538  5.813  100.0  4.0952   4.0  307.0   \n",
       "24    0.75026   0.0   8.14   0.0  0.538  5.924   94.1  4.3996   4.0  307.0   \n",
       "25    0.84054   0.0   8.14   0.0  0.538  5.599   85.7  4.4546   4.0  307.0   \n",
       "26    0.67191   0.0   8.14   0.0  0.538  5.813   90.3  4.6820   4.0  307.0   \n",
       "27    0.95577   0.0   8.14   0.0  0.538  6.047   88.8  4.4534   4.0  307.0   \n",
       "28    0.77299   0.0   8.14   0.0  0.538  6.495   94.4  4.4547   4.0  307.0   \n",
       "29    1.00245   0.0   8.14   0.0  0.538  6.674   87.3  4.2390   4.0  307.0   \n",
       "..        ...   ...    ...   ...    ...    ...    ...     ...   ...    ...   \n",
       "476   4.87141   0.0  18.10   0.0  0.614  6.484   93.6  2.3053  24.0  666.0   \n",
       "477  15.02340   0.0  18.10   0.0  0.614  5.304   97.3  2.1007  24.0  666.0   \n",
       "478  10.23300   0.0  18.10   0.0  0.614  6.185   96.7  2.1705  24.0  666.0   \n",
       "479  14.33370   0.0  18.10   0.0  0.614  6.229   88.0  1.9512  24.0  666.0   \n",
       "480   5.82401   0.0  18.10   0.0  0.532  6.242   64.7  3.4242  24.0  666.0   \n",
       "481   5.70818   0.0  18.10   0.0  0.532  6.750   74.9  3.3317  24.0  666.0   \n",
       "482   5.73116   0.0  18.10   0.0  0.532  7.061   77.0  3.4106  24.0  666.0   \n",
       "483   2.81838   0.0  18.10   0.0  0.532  5.762   40.3  4.0983  24.0  666.0   \n",
       "484   2.37857   0.0  18.10   0.0  0.583  5.871   41.9  3.7240  24.0  666.0   \n",
       "485   3.67367   0.0  18.10   0.0  0.583  6.312   51.9  3.9917  24.0  666.0   \n",
       "486   5.69175   0.0  18.10   0.0  0.583  6.114   79.8  3.5459  24.0  666.0   \n",
       "487   4.83567   0.0  18.10   0.0  0.583  5.905   53.2  3.1523  24.0  666.0   \n",
       "488   0.15086   0.0  27.74   0.0  0.609  5.454   92.7  1.8209   4.0  711.0   \n",
       "489   0.18337   0.0  27.74   0.0  0.609  5.414   98.3  1.7554   4.0  711.0   \n",
       "490   0.20746   0.0  27.74   0.0  0.609  5.093   98.0  1.8226   4.0  711.0   \n",
       "491   0.10574   0.0  27.74   0.0  0.609  5.983   98.8  1.8681   4.0  711.0   \n",
       "492   0.11132   0.0  27.74   0.0  0.609  5.983   83.5  2.1099   4.0  711.0   \n",
       "493   0.17331   0.0   9.69   0.0  0.585  5.707   54.0  2.3817   6.0  391.0   \n",
       "494   0.27957   0.0   9.69   0.0  0.585  5.926   42.6  2.3817   6.0  391.0   \n",
       "495   0.17899   0.0   9.69   0.0  0.585  5.670   28.8  2.7986   6.0  391.0   \n",
       "496   0.28960   0.0   9.69   0.0  0.585  5.390   72.9  2.7986   6.0  391.0   \n",
       "497   0.26838   0.0   9.69   0.0  0.585  5.794   70.6  2.8927   6.0  391.0   \n",
       "498   0.23912   0.0   9.69   0.0  0.585  6.019   65.3  2.4091   6.0  391.0   \n",
       "499   0.17783   0.0   9.69   0.0  0.585  5.569   73.5  2.3999   6.0  391.0   \n",
       "500   0.22438   0.0   9.69   0.0  0.585  6.027   79.7  2.4982   6.0  391.0   \n",
       "501   0.06263   0.0  11.93   0.0  0.573  6.593   69.1  2.4786   1.0  273.0   \n",
       "502   0.04527   0.0  11.93   0.0  0.573  6.120   76.7  2.2875   1.0  273.0   \n",
       "503   0.06076   0.0  11.93   0.0  0.573  6.976   91.0  2.1675   1.0  273.0   \n",
       "504   0.10959   0.0  11.93   0.0  0.573  6.794   89.3  2.3889   1.0  273.0   \n",
       "505   0.04741   0.0  11.93   0.0  0.573  6.030   80.8  2.5050   1.0  273.0   \n",
       "\n",
       "     PTRATIO       B  LSTAT  \n",
       "0       15.3  396.90   4.98  \n",
       "1       17.8  396.90   9.14  \n",
       "2       17.8  392.83   4.03  \n",
       "3       18.7  394.63   2.94  \n",
       "4       18.7  396.90   5.33  \n",
       "5       18.7  394.12   5.21  \n",
       "6       15.2  395.60  12.43  \n",
       "7       15.2  396.90  19.15  \n",
       "8       15.2  386.63  29.93  \n",
       "9       15.2  386.71  17.10  \n",
       "10      15.2  392.52  20.45  \n",
       "11      15.2  396.90  13.27  \n",
       "12      15.2  390.50  15.71  \n",
       "13      21.0  396.90   8.26  \n",
       "14      21.0  380.02  10.26  \n",
       "15      21.0  395.62   8.47  \n",
       "16      21.0  386.85   6.58  \n",
       "17      21.0  386.75  14.67  \n",
       "18      21.0  288.99  11.69  \n",
       "19      21.0  390.95  11.28  \n",
       "20      21.0  376.57  21.02  \n",
       "21      21.0  392.53  13.83  \n",
       "22      21.0  396.90  18.72  \n",
       "23      21.0  394.54  19.88  \n",
       "24      21.0  394.33  16.30  \n",
       "25      21.0  303.42  16.51  \n",
       "26      21.0  376.88  14.81  \n",
       "27      21.0  306.38  17.28  \n",
       "28      21.0  387.94  12.80  \n",
       "29      21.0  380.23  11.98  \n",
       "..       ...     ...    ...  \n",
       "476     20.2  396.21  18.68  \n",
       "477     20.2  349.48  24.91  \n",
       "478     20.2  379.70  18.03  \n",
       "479     20.2  383.32  13.11  \n",
       "480     20.2  396.90  10.74  \n",
       "481     20.2  393.07   7.74  \n",
       "482     20.2  395.28   7.01  \n",
       "483     20.2  392.92  10.42  \n",
       "484     20.2  370.73  13.34  \n",
       "485     20.2  388.62  10.58  \n",
       "486     20.2  392.68  14.98  \n",
       "487     20.2  388.22  11.45  \n",
       "488     20.1  395.09  18.06  \n",
       "489     20.1  344.05  23.97  \n",
       "490     20.1  318.43  29.68  \n",
       "491     20.1  390.11  18.07  \n",
       "492     20.1  396.90  13.35  \n",
       "493     19.2  396.90  12.01  \n",
       "494     19.2  396.90  13.59  \n",
       "495     19.2  393.29  17.60  \n",
       "496     19.2  396.90  21.14  \n",
       "497     19.2  396.90  14.10  \n",
       "498     19.2  396.90  12.92  \n",
       "499     19.2  395.77  15.10  \n",
       "500     19.2  396.90  14.33  \n",
       "501     21.0  391.99   9.67  \n",
       "502     21.0  396.90   9.08  \n",
       "503     21.0  396.90   5.64  \n",
       "504     21.0  393.45   6.48  \n",
       "505     21.0  396.90   7.88  \n",
       "\n",
       "[506 rows x 13 columns]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boston"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "price = boston_data.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(506,)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "price.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24.0"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "price[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data, test_data , train_result, test_result = train_test_split(boston,price,test_size=0.3)\n",
    "#train_data & test_data only includes feature info not price info\n",
    "#price_info info is present in train_result & test_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = LinearRegression(fit_intercept=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_data,train_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_predict = model.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method score in module sklearn.base:\n",
      "\n",
      "score(X, y, sample_weight=None) method of sklearn.linear_model.base.LinearRegression instance\n",
      "    Returns the coefficient of determination R^2 of the prediction.\n",
      "    \n",
      "    The coefficient R^2 is defined as (1 - u/v), where u is the regression\n",
      "    sum of squares ((y_true - y_pred) ** 2).sum() and v is the residual\n",
      "    sum of squares ((y_true - y_true.mean()) ** 2).sum().\n",
      "    Best possible score is 1.0 and it can be negative (because the\n",
      "    model can be arbitrarily worse). A constant model that always\n",
      "    predicts the expected value of y, disregarding the input features,\n",
      "    would get a R^2 score of 0.0.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    X : array-like, shape = (n_samples, n_features)\n",
      "        Test samples.\n",
      "    \n",
      "    y : array-like, shape = (n_samples) or (n_samples, n_outputs)\n",
      "        True values for X.\n",
      "    \n",
      "    sample_weight : array-like, shape = [n_samples], optional\n",
      "        Sample weights.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    score : float\n",
      "        R^2 of self.predict(X) wrt. y.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(model.score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ZekeLabs\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:395: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "shapes (1,152) and (13,) not aligned: 152 (dim 1) != 13 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m-----------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-124-349b076ad341>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_result\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_predict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Users\\ZekeLabs\\Anaconda3\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36mscore\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    384\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    385\u001b[0m         \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mr2_score\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 386\u001b[1;33m         return r2_score(y, self.predict(X), sample_weight=sample_weight,\n\u001b[0m\u001b[0;32m    387\u001b[0m                         multioutput='variance_weighted')\n\u001b[0;32m    388\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\ZekeLabs\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\base.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    266\u001b[0m             \u001b[0mReturns\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    267\u001b[0m         \"\"\"\n\u001b[1;32m--> 268\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_decision_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    269\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    270\u001b[0m     \u001b[0m_preprocess_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstaticmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_preprocess_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\ZekeLabs\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\base.py\u001b[0m in \u001b[0;36m_decision_function\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    251\u001b[0m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'csr'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'csc'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'coo'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    252\u001b[0m         return safe_sparse_dot(X, self.coef_.T,\n\u001b[1;32m--> 253\u001b[1;33m                                dense_output=True) + self.intercept_\n\u001b[0m\u001b[0;32m    254\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    255\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\ZekeLabs\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\extmath.py\u001b[0m in \u001b[0;36msafe_sparse_dot\u001b[1;34m(a, b, dense_output)\u001b[0m\n\u001b[0;32m    187\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    188\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 189\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfast_dot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    190\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    191\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: shapes (1,152) and (13,) not aligned: 152 (dim 1) != 13 (dim 0)"
     ]
    }
   ],
   "source": [
    "model.score(test_result,y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(152,)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "continuous is not supported",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m-----------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-129-1caf18a4c242>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0maccuracy_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_result\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_predict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Users\\ZekeLabs\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py\u001b[0m in \u001b[0;36maccuracy_score\u001b[1;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[0;32m    170\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    171\u001b[0m     \u001b[1;31m# Compute accuracy for each possible representation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 172\u001b[1;33m     \u001b[0my_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    173\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0my_type\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'multilabel'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    174\u001b[0m         \u001b[0mdiffering_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcount_nonzero\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\ZekeLabs\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m     87\u001b[0m     \u001b[1;31m# No metrics support \"multiclass-multioutput\" format\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0my_type\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m\"binary\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"multiclass\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"multilabel-indicator\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 89\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"{0} is not supported\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     90\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0my_type\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m\"binary\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"multiclass\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: continuous is not supported"
     ]
    }
   ],
   "source": [
    "accuracy_score(test_result,y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
